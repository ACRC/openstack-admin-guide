
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Operations and Monitoring &#8212; OpenStack Administration Guide  documentation</title>
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Customising the OpenStack Deployment" href="customising_deployment.html" />
    <link rel="prev" title="Managing Users and Projects" href="managing_users_and_projects.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="operations-and-monitoring">
<h1>Operations and Monitoring<a class="headerlink" href="#operations-and-monitoring" title="Permalink to this headline">¶</a></h1>
<div class="section" id="access-to-kibana">
<h2>Access to Kibana<a class="headerlink" href="#access-to-kibana" title="Permalink to this headline">¶</a></h2>
<p>OpenStack control plane logs are aggregated from all servers by Monasca and
stored in ElasticSearch. The control plane logs can be accessed from
ElasticSearch using Kibana, which is available at the following URL:
<a class="reference external" href="https://openstack.acme.example:5601">https://openstack.acme.example:5601</a></p>
<p>To login, use the <code class="docutils literal notranslate"><span class="pre">kibana</span></code> user. The password is auto-generated by
Kolla-Ansible and can be extracted from the encrypted passwords file
(<a class="reference external" href="https://github.com/acme-openstack/kayobe-config/blob/acme/train/etc/kayobe/kolla/passwords.yml">https://github.com/acme-openstack/kayobe-config/blob/acme/train/etc/kayobe/kolla/passwords.yml</a>):</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kayobe# ansible-vault view ${KAYOBE_CONFIG_PATH}/kolla/passwords.yml --vault-password-file ~/vault-password | grep ^kibana</span>
</pre></div>
</div>
</div>
<div class="section" id="access-to-grafana">
<h2>Access to Grafana<a class="headerlink" href="#access-to-grafana" title="Permalink to this headline">¶</a></h2>
<p>Monasca metrics can be visualised in Grafana dashboards. Monasca Grafana can be
found at the following address: <a class="reference external" href="https://openstack.acme.example:3000">https://openstack.acme.example:3000</a></p>
<p>Grafana uses Keystone authentication. To login, use valid OpenStack user
credentials.</p>
<p>To visualise control plane metrics, you will need one of the following roles in
the <code class="docutils literal notranslate"><span class="pre">monasca_control_plane</span></code> project:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">admin</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">monasca-user</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">monasca-read-only-user</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">monasca-editor</span></code></p></li>
</ul>
</div>
<div class="section" id="migrating-virtual-machines">
<h2>Migrating virtual machines<a class="headerlink" href="#migrating-virtual-machines" title="Permalink to this headline">¶</a></h2>
<p>To see where all virtual machines are running on the hypervisors:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">admin# openstack server list --all-projects --long</span>
</pre></div>
</div>
<p>To move a virtual machine with shared storage or booted from volume from one hypervisor to another, for example to
<code class="docutils literal notranslate"><span class="pre">comp0</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">admin# openstack --os-compute-api-version 2.30 server migrate --live-migration --host comp0 6a35592c-5a7e-4da3-9ab9-6765345641cb</span>
</pre></div>
</div>
<p>To move a virtual machine with local disks:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">admin# openstack --os-compute-api-version 2.30 server migrate --live-migration --block-migration --host comp0 6a35592c-5a7e-4da3-9ab9-6765345641cb</span>
</pre></div>
</div>
</div>
<div class="section" id="openstack-reconfiguration">
<h2>OpenStack Reconfiguration<a class="headerlink" href="#openstack-reconfiguration" title="Permalink to this headline">¶</a></h2>
<div class="section" id="disabling-a-service">
<h3>Disabling a Service<a class="headerlink" href="#disabling-a-service" title="Permalink to this headline">¶</a></h3>
<p>Ansible is oriented towards adding or reconfiguring services, but removing a
service is handled less well, because of Ansible’s imperative style.</p>
<p>To remove a service, it is disabled in Kayobe’s Kolla config, which prevents
other services from communicating with it. For example, to disable
<code class="docutils literal notranslate"><span class="pre">cinder-backup</span></code>, edit <code class="docutils literal notranslate"><span class="pre">${KAYOBE_CONFIG_PATH}/kolla.yml</span></code>:</p>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="gd">-enable_cinder_backup: true</span>
<span class="gi">+enable_cinder_backup: false</span>
</pre></div>
</div>
<p>Then, reconfigure Cinder services with Kayobe:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kayobe# kayobe overcloud service reconfigure --kolla-tags cinder</span>
</pre></div>
</div>
<p>However, the service itself, no longer in Ansible’s manifest of managed state,
must be manually stopped and prevented from restarting.</p>
<p>On each controller:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kayobe# docker rm -f cinder_backup</span>
</pre></div>
</div>
<p>Some services may store data in a dedicated Docker volume, which can be removed
with <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">volume</span> <span class="pre">rm</span></code>.</p>
</div>
<div class="section" id="installing-and-updating-tls-certificates">
<h3>Installing and Updating TLS Certificates<a class="headerlink" href="#installing-and-updating-tls-certificates" title="Permalink to this headline">¶</a></h3>
<p>TLS is implemented using a wildcard certificate available for <code class="docutils literal notranslate"><span class="pre">*.acme.example</span></code>.</p>
<p>To configure TLS for the first time, we write a PEM file to the <code class="docutils literal notranslate"><span class="pre">secrets.yml</span></code>
file as <code class="docutils literal notranslate"><span class="pre">secrets_kolla_external_tls_cert</span></code>. Use a command of this form:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kayobe# ansible-vault edit ${KAYOBE_CONFIG_PATH}/secrets.yml --vault-password-file=~/vault-password</span>
</pre></div>
</div>
<p>Concatenate the contents of the certificate and key files to create
<code class="docutils literal notranslate"><span class="pre">secrets_kolla_external_tls_cert</span></code>.</p>
<p>In <code class="docutils literal notranslate"><span class="pre">${KAYOBE_CONFIG_PATH}/kolla.yml</span></code>, set the following:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">kolla_enable_tls_external</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">True</span>
<span class="nt">kolla_external_tls_cert</span><span class="p">:</span> <span class="s">&quot;{{</span><span class="nv"> </span><span class="s">secrets_kolla_external_tls_cert</span><span class="nv"> </span><span class="s">}}&quot;</span>
</pre></div>
</div>
<p>To configure TLS, we need to reconfigure all services, as endpoint URLs need to
be updated in Keystone:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kayobe# kayobe overcloud service reconfigure</span>
</pre></div>
</div>
<p>To update an existing certificate, for example when it has reached expiration,
change the value of <code class="docutils literal notranslate"><span class="pre">secrets_kolla_external_tls_cert</span></code> and run the following
command:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kayobe# kayobe overcloud service reconfigure --kolla-tags haproxy</span>
</pre></div>
</div>
</div>
<div class="section" id="taking-a-hypervisor-out-of-service">
<span id="id1"></span><h3>Taking a Hypervisor out of Service<a class="headerlink" href="#taking-a-hypervisor-out-of-service" title="Permalink to this headline">¶</a></h3>
<p>To take a hypervisor out of Nova scheduling, for example <code class="docutils literal notranslate"><span class="pre">comp0</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">admin# openstack compute service set --disable \</span>
<span class="go">       comp0 nova-compute</span>
</pre></div>
</div>
<p>Running instances on the hypervisor will not be affected, but new instances
will not be deployed on it.</p>
<p>A reason for disabling a hypervisor can be documented with the
<code class="docutils literal notranslate"><span class="pre">--disable-reason</span></code> flag:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">admin# openstack compute service set --disable \</span>
<span class="go">       --disable-reason &quot;Broken drive&quot; comp0 nova-compute</span>
</pre></div>
</div>
<p>Details about all hypervisors and the reasons they are disabled can be
displayed with:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">admin# openstack compute service list --long</span>
</pre></div>
</div>
<p>And then to enable a hypervisor again:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">admin# openstack compute service set --enable \</span>
<span class="go">       comp0 nova-compute</span>
</pre></div>
</div>
</div>
<div class="section" id="managing-space-in-the-docker-registry">
<h3>Managing Space in the Docker Registry<a class="headerlink" href="#managing-space-in-the-docker-registry" title="Permalink to this headline">¶</a></h3>
<p>If the Docker registry becomes full, this can prevent container updates and
(depending on the storage configuration of the seed host) could lead to other
problems with services provided by the seed host.</p>
<p>To remove container images from the Docker Registry, follow this process:</p>
<ul class="simple">
<li><p>Reconfigure the registry container to allow deleting containers. This can be
done in <code class="docutils literal notranslate"><span class="pre">docker-registry.yml</span></code> with Kayobe:</p></li>
</ul>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">docker_registry_env</span><span class="p">:</span>
  <span class="nt">REGISTRY_STORAGE_DELETE_ENABLED</span><span class="p">:</span> <span class="s">&quot;true&quot;</span>
</pre></div>
</div>
<ul class="simple">
<li><p>For the change to take effect, run:</p></li>
</ul>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kayobe seed host configure</span>
</pre></div>
</div>
<ul class="simple">
<li><p>A helper script is useful, such as <a class="reference external" href="https://github.com/byrnedo/docker-reg-tool">https://github.com/byrnedo/docker-reg-tool</a>
(this requires <code class="docutils literal notranslate"><span class="pre">jq</span></code>). To delete all images with a specific tag, use:</p></li>
</ul>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">for repo in `./docker_reg_tool http://registry-ip:4000 list`; do</span>
<span class="go">     ./docker_reg_tool http://registry-ip:4000 delete $repo $tag</span>
<span class="go">done</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Deleting the tag does not actually release the space. To actually free up
space, run garbage collection:</p></li>
</ul>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">seed# docker exec docker_registry bin/registry garbage-collect /etc/docker/registry/config.yml</span>
</pre></div>
</div>
<p>The seed host can also accrue a lot of data from building container images.
The images stored locally in the seed host can be seen using <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">image</span> <span class="pre">ls</span></code>.</p>
<p>Old and redundant images can be identified from their names and tags, and
removed using <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">image</span> <span class="pre">rm</span></code>.</p>
</div>
</div>
<div class="section" id="backup-of-the-openstack-control-plane">
<h2>Backup of the OpenStack Control Plane<a class="headerlink" href="#backup-of-the-openstack-control-plane" title="Permalink to this headline">¶</a></h2>
<p>As the backup procedure is constantly changing, it is normally best to check
the upstream documentation for an up to date procedure. Here is a high level
overview of the key things you need to backup:</p>
<div class="section" id="controllers">
<h3>Controllers<a class="headerlink" href="#controllers" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.openstack.org/kayobe/latest/administration/overcloud.html#performing-database-backups">Back up SQL databases</a></p></li>
<li><p><a class="reference external" href="https://docs.openstack.org/kayobe/latest/administration/overcloud.html#saving-overcloud-service-configuration">Back up configuration in /etc/kolla</a></p></li>
</ul>
</div>
<div class="section" id="compute">
<h3>Compute<a class="headerlink" href="#compute" title="Permalink to this headline">¶</a></h3>
<p>The compute nodes can largely be thought of as ephemeral, but you do need to
make sure you have migrated any instances and disabled the hypervisor before
decommissioning or making any disruptive configuration change.</p>
</div>
<div class="section" id="monitoring">
<h3>Monitoring<a class="headerlink" href="#monitoring" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.influxdata.com/influxdb/v1.8/administration/backup_and_restore/">Back up InfluxDB</a></p></li>
<li><p><a class="reference external" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/backup-cluster-data.html">Back up ElasticSearch</a></p></li>
</ul>
</div>
<div class="section" id="seed">
<h3>Seed<a class="headerlink" href="#seed" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.openstack.org/kayobe/latest/administration/seed.html#database-backup-restore">Back up bifrost</a></p></li>
</ul>
</div>
<div class="section" id="ansible-control-host">
<h3>Ansible control host<a class="headerlink" href="#ansible-control-host" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Back up service VMs such as the seed VM</p></li>
</ul>
</div>
</div>
<div class="section" id="control-plane-monitoring">
<h2>Control Plane Monitoring<a class="headerlink" href="#control-plane-monitoring" title="Permalink to this headline">¶</a></h2>
<p>Monasca has been configured to collect logs and metrics across the control
plane. It provides a single point where control plane monitoring and telemetry
data can be analysed and correlated.</p>
<p>Metrics are collected per server via the <a class="reference external" href="https://opendev.org/openstack/monasca-agent">Monasca Agent</a>. The Monasca Agent is deployed
and configured by Kolla Ansible.</p>
<p>Logging to Monasca is done via a <a class="reference external" href="https://github.com/monasca/fluentd-monasca">Fluentd output plugin</a>.</p>
<div class="section" id="configuring-monasca-alerts">
<h3>Configuring Monasca Alerts<a class="headerlink" href="#configuring-monasca-alerts" title="Permalink to this headline">¶</a></h3>
<div class="section" id="generating-metrics-from-specific-log-messages">
<h4>Generating Metrics from Specific Log Messages<a class="headerlink" href="#generating-metrics-from-specific-log-messages" title="Permalink to this headline">¶</a></h4>
<p>If you wish to generate alerts for specific log messages, you must first
generate metrics from those log messages. Metrics are generated from the
transformed logs queue in Kafka. The Monasca log metrics service reads log
messages from this queue, transforms them into metrics and then writes them to
the metrics queue.</p>
<p>The rules which govern this transformation are defined in the logstash config
file. This file can be configured via kayobe. To do this, edit
<code class="docutils literal notranslate"><span class="pre">etc/kayobe/kolla/config/monasca/log-metrics.conf</span></code>, for example:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span># Create events from specific log signatures
filter {
  if &quot;Another thread already created a resource provider&quot; in [log][message] {
    mutate {
      add_field =&gt; { &quot;[log][dimensions][event]&quot; =&gt; &quot;hat&quot; }
    }
  } else if &quot;My string here&quot; in [log][message] {
    mutate {
      add_field =&gt; { &quot;[log][dimensions][event]&quot; =&gt; &quot;my_new_alert&quot; }
    }
 }
</pre></div>
</div>
<p>Reconfigure Monasca:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>kayobe# kayobe overcloud service reconfigure --kolla-tags monasca
</pre></div>
</div>
<p>Verify that logstash doesn’t complain about your modification. On each node
running the <code class="docutils literal notranslate"><span class="pre">monasca-log-metrics</span></code> service, the logs can be inspected in the
Kolla logs directory, under the <code class="docutils literal notranslate"><span class="pre">logstash</span></code> folder:
<code class="docutils literal notranslate"><span class="pre">/var/log/kolla/logstash</span></code>.</p>
<p>Metrics will now be generated from the configured log messages. To generate
alerts/notifications from your new metric, follow the next section.</p>
</div>
<div class="section" id="generating-monasca-alerts-from-metrics">
<h4>Generating Monasca Alerts from Metrics<a class="headerlink" href="#generating-monasca-alerts-from-metrics" title="Permalink to this headline">¶</a></h4>
<p>Firstly, we will configure alarms and notifications. This should be done via
the Monasca client. More detailed documentation is available in the <a class="reference external" href="https://github.com/openstack/monasca-api/blob/master/docs/monasca-api-spec.md#alarm-definitions-and-alarms">Monasca
API specification</a>.
This document provides an overview of common use-cases.</p>
<p>To create a Slack notification, first obtain the URL for the notification hook
from Slack, and configure the notification as follows:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">monasca# monasca notification-create stackhpc_slack SLACK https://hooks.slack.com/services/UUID</span>
</pre></div>
</div>
<p>You can view notifications at any time by invoking:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">monasca# monasca notification-list</span>
</pre></div>
</div>
<p>To create an alarm with an associated notification:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">monasca# monasca alarm-definition-create multiple_nova_compute \</span>
<span class="go">         &#39;(count(log.event.multiple_nova_compute{}, deterministic)&gt;0)&#39; \</span>
<span class="go">         --description &quot;Multiple nova compute instances detected&quot; \</span>
<span class="go">         --severity HIGH --alarm-actions $NOTIFICATION_ID</span>
</pre></div>
</div>
<p>By default one alarm will be created for all hosts. This is typically useful
when you are looking at the overall state of some hosts. For example in the
screenshot below the <code class="docutils literal notranslate"><span class="pre">db_mon_log_high_mem_usage</span></code> alarm has previously
triggered on a number of hosts, but is currently below threshold.</p>
<p>If you wish to have an alarm created per host you can use the <code class="docutils literal notranslate"><span class="pre">--match-by</span></code>
option and specify the hostname dimension. For example:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">monasca# monasca alarm-definition-create multiple_nova_compute \</span>
<span class="go">         &#39;(count(log.event.multiple_nova_compute{}, deterministic)&gt;0)&#39; \</span>
<span class="go">         --description &quot;Multiple nova compute instances detected&quot; \</span>
<span class="go">         --severity HIGH --alarm-actions $NOTIFICATION_ID</span>
<span class="go">         --match-by hostname</span>
</pre></div>
</div>
<p>Creating an alarm per host can be useful when alerting on one off events such
as log messages which need to be actioned individually. Once the issue has been
investigated and fixed, the alarm can be deleted on a per host basis.</p>
<p>For example, in the case of monitoring for file system corruption one might
define a metric from the system logs alerting on XFS file system corruption, or
ECC memory errors. These metrics may only be generated once, but it is
important that they are not ignored. Therefore, in the example below, the last
operator is used so that the alarm is evaluated against the last metric
associated with the log message. Since for log metrics the value of this metric
is always greater than 0, this alarm can only be reset by deleting it (which
can be accomplished by clicking on the dustbin icon in Monasca Grafana). By
ensuring that the alarm has to be manually deleted and will not reset to the OK
status, important errors can be tracked.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">monasca# monasca alarm-definition-create xfs_errors \</span>
<span class="go">         &#39;(last(log.event.xfs_errors_detected{}, deterministic)&gt;0)&#39; \</span>
<span class="go">         --description &quot;XFS errors detected on host&quot; \</span>
<span class="go">         --severity HIGH --alarm-actions $NOTIFICATION_ID \</span>
<span class="go">         --match-by hostname</span>
</pre></div>
</div>
<p>It is also possible to update existing alarms. For example, to update, or add
multiple notifications to an alarm:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">monasca# monasca alarm-definition-patch $ALARM_ID --alarm-actions $NOTIFICATION_ID --alarm-actions $NOTIFICATION_ID_2</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="control-plane-shutdown-procedure">
<h2>Control Plane Shutdown Procedure<a class="headerlink" href="#control-plane-shutdown-procedure" title="Permalink to this headline">¶</a></h2>
<div class="section" id="overview">
<h3>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Verify integrity of clustered components (RabbitMQ, Galera, Keepalived). They
should all report a healthy status.</p></li>
<li><p>Put node into maintenance mode in bifrost to prevent it from automatically
powering back on</p></li>
<li><p>Shutdown down nodes one at a time gracefully using systemctl poweroff</p></li>
</ul>
</div>
<div class="section" id="id2">
<h3>Controllers<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>If you are restarting the controllers, it is best to do this one controller at
a time to avoid the clustered components losing quorum.</p>
<div class="section" id="checking-galera-state">
<h4>Checking Galera state<a class="headerlink" href="#checking-galera-state" title="Permalink to this headline">¶</a></h4>
<p>On each controller perform the following:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">[stack@ctrl0 ~]$ </span>docker <span class="nb">exec</span> -i mariadb mysql -u root -p -e <span class="s2">&quot;SHOW STATUS LIKE &#39;wsrep_local_state_comment&#39;&quot;</span>
<span class="go">Variable_name   Value</span>
<span class="go">wsrep_local_state_comment       Synced</span>
</pre></div>
</div>
<p>The password can be found using:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kayobe# ansible-vault view ${KAYOBE_CONFIG_PATH}/kolla/passwords.yml \</span>
<span class="go">        --vault-password-file ~/vault-password | grep ^database</span>
</pre></div>
</div>
</div>
<div class="section" id="checking-rabbitmq">
<h4>Checking RabbitMQ<a class="headerlink" href="#checking-rabbitmq" title="Permalink to this headline">¶</a></h4>
<p>RabbitMQ health is determined using the command <code class="docutils literal notranslate"><span class="pre">rabbitmqctl</span> <span class="pre">cluster_status</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">[stack@ctrl0 ~]$ </span>docker <span class="nb">exec</span> rabbitmq rabbitmqctl cluster_status
<span class="go">Cluster status of node rabbit@ctrl0 ...</span>
<span class="go">[{nodes,[{disc,[&#39;rabbit@ctrl0&#39;,&#39;rabbit@ctrl1&#39;,</span>
<span class="go">                &#39;rabbit@ctr2&#39;]}]},</span>
<span class="go"> {running_nodes,[&#39;rabbit@ctrl1&#39;,&#39;rabbit@ctr2&#39;,</span>
<span class="go">                 &#39;rabbit@ctrl0&#39;]},</span>
<span class="go"> {cluster_name,&lt;&lt;&quot;rabbit@ctr2&quot;&gt;&gt;},</span>
<span class="go"> {partitions,[]},</span>
<span class="go"> {alarms,[{&#39;rabbit@ctrl1&#39;,[]},</span>
<span class="go">          {&#39;rabbit@ctr2&#39;,[]},</span>
<span class="go">          {&#39;rabbit@ctrl0&#39;,[]}]}]</span>
</pre></div>
</div>
</div>
<div class="section" id="checking-keepalived">
<h4>Checking Keepalived<a class="headerlink" href="#checking-keepalived" title="Permalink to this headline">¶</a></h4>
<p>On (for example) three controllers:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">[stack@ctrl0 ~]$ </span>docker logs keepalived
</pre></div>
</div>
<p>Two instances should show:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">VRRP_Instance(kolla_internal_vip_51) Entering BACKUP STATE</span>
</pre></div>
</div>
<p>and the other:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">VRRP_Instance(kolla_internal_vip_51) Entering MASTER STATE</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id3">
<h3>Ansible Control Host<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>The Ansible control host is not enrolled in bifrost. This node may run services
such as the seed virtual machine which will need to be gracefully powered down.</p>
</div>
<div class="section" id="id4">
<h3>Compute<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>If you are shutting down a single hypervisor, to avoid down time to tenants it
is advisable to migrate all of the instances to another machine. See
<a class="reference internal" href="hardware_inventory_management.html#evacuating-all-instances"><span class="std std-ref">Evacuating all instances</span></a>.</p>
</div>
<div class="section" id="shutting-down-the-seed-vm">
<h3>Shutting down the seed VM<a class="headerlink" href="#shutting-down-the-seed-vm" title="Permalink to this headline">¶</a></h3>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kayobe# ssh stack@acme-seed sudo systemctl poweroff</span>
<span class="go">kayobe# virsh shutdown acme-seed</span>
</pre></div>
</div>
</div>
<div class="section" id="full-shutdown">
<span id="id5"></span><h3>Full shutdown<a class="headerlink" href="#full-shutdown" title="Permalink to this headline">¶</a></h3>
<p>In case a full shutdown of the system is required, we advise to use the
following order:</p>
<ul class="simple">
<li><p>Perform a graceful shutdown of all virtual machine instances</p></li>
<li><p>Shut down compute nodes</p></li>
<li><p>Shut down monitoring node</p></li>
<li><p>Shut down network nodes (if separate from controllers)</p></li>
<li><p>Shut down controllers</p></li>
<li><p>Shut down Ceph nodes (if applicable)</p></li>
<li><p>Shut down seed VM</p></li>
<li><p>Shut down Ansible control host</p></li>
</ul>
</div>
<div class="section" id="rebooting-a-node">
<h3>Rebooting a node<a class="headerlink" href="#rebooting-a-node" title="Permalink to this headline">¶</a></h3>
<p>Example: Reboot all over the computes apart from <code class="docutils literal notranslate"><span class="pre">comp0</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kayobe# kayobe overcloud host command run --limit &#39;compute:!comp0&#39; -b --command &quot;shutdown -r&quot;</span>
</pre></div>
</div>
</div>
<div class="section" id="references">
<h3>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://galeracluster.com/library/training/tutorials/restarting-cluster.html">https://galeracluster.com/library/training/tutorials/restarting-cluster.html</a></p></li>
</ul>
</div>
</div>
<div class="section" id="control-plane-power-on-procedure">
<h2>Control Plane Power on Procedure<a class="headerlink" href="#control-plane-power-on-procedure" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id6">
<h3>Overview<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Remove the node from maintenance mode in bifrost</p></li>
<li><p>Bifrost should automatically power on the node via IPMI</p></li>
<li><p>Check that all docker containers are running</p></li>
<li><p>Check Kibana for any messages with log level ERROR or equivalent</p></li>
</ul>
</div>
<div class="section" id="id7">
<h3>Controllers<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<p>If all of the servers were shut down at the same time, it is necessary to run a
script to recover the database once they have all started up. This can be done
with the following command:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kayobe# kayobe overcloud database recover</span>
</pre></div>
</div>
</div>
<div class="section" id="id8">
<h3>Ansible Control Host<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<p>The Ansible control host is not enrolled in Bifrost and will have to be powered
on manually.</p>
</div>
<div class="section" id="seed-vm">
<h3>Seed VM<a class="headerlink" href="#seed-vm" title="Permalink to this headline">¶</a></h3>
<p>The seed VM (and any other service VM) should start automatically when the seed
hypervisor is powered on. If it does not, it can be started with:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kayobe# virsh start seed-0</span>
</pre></div>
</div>
</div>
<div class="section" id="full-power-on">
<h3>Full power on<a class="headerlink" href="#full-power-on" title="Permalink to this headline">¶</a></h3>
<p>Follow the order in <a class="reference internal" href="#full-shutdown"><span class="std std-ref">Full shutdown</span></a>, but in reverse order.</p>
</div>
<div class="section" id="shutting-down-restarting-monitoring-services">
<h3>Shutting Down / Restarting Monitoring Services<a class="headerlink" href="#shutting-down-restarting-monitoring-services" title="Permalink to this headline">¶</a></h3>
<div class="section" id="shutting-down">
<h4>Shutting down<a class="headerlink" href="#shutting-down" title="Permalink to this headline">¶</a></h4>
<p>Log into the monitoring host(s):</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kayobe# ssh stack@mon0</span>
</pre></div>
</div>
<p>Stop all Docker containers:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">mon0# for i in `docker ps -q`; do docker stop $i; done</span>
</pre></div>
</div>
<p>Shut down the node:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">mon0# sudo shutdown -h</span>
</pre></div>
</div>
</div>
<div class="section" id="starting-up">
<h4>Starting up<a class="headerlink" href="#starting-up" title="Permalink to this headline">¶</a></h4>
<p>The monitoring services containers will automatically start when the monitoring
node is powered back on.</p>
</div>
</div>
</div>
<div class="section" id="software-updates">
<h2>Software Updates<a class="headerlink" href="#software-updates" title="Permalink to this headline">¶</a></h2>
<div class="section" id="update-packages-on-control-plane">
<h3>Update Packages on Control Plane<a class="headerlink" href="#update-packages-on-control-plane" title="Permalink to this headline">¶</a></h3>
<p>OS packages can be updated with:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kayobe # kayobe overcloud host package update --limit comp0 --packages &#39;*&#39;</span>
<span class="go">kayobe # kayobe overcloud seed package update --packages &#39;*&#39;</span>
</pre></div>
</div>
<p>See <a class="reference external" href="https://docs.openstack.org/kayobe/latest/administration/overcloud.html#updating-packages">https://docs.openstack.org/kayobe/latest/administration/overcloud.html#updating-packages</a></p>
</div>
<div class="section" id="minor-upgrades-to-openstack-services">
<h3>Minor Upgrades to OpenStack Services<a class="headerlink" href="#minor-upgrades-to-openstack-services" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Pull latest changes from upstream stable branch to your own <code class="docutils literal notranslate"><span class="pre">kolla</span></code> fork (if applicable)</p></li>
<li><p>Update <code class="docutils literal notranslate"><span class="pre">kolla_openstack_release</span></code> in <code class="docutils literal notranslate"><span class="pre">etc/kayobe/kolla.yml</span></code> (unless using default)</p></li>
<li><p>Update tags for the images in <code class="docutils literal notranslate"><span class="pre">etc/kayobe/kolla/globals.yml</span></code> to use the new value of <code class="docutils literal notranslate"><span class="pre">kolla_openstack_release</span></code></p></li>
<li><p>Rebuild container images</p></li>
<li><p>Pull container images to overcloud hosts</p></li>
<li><p>Run kayobe overcloud service upgrade</p></li>
</ul>
<p>For more information, see: <a class="reference external" href="https://docs.openstack.org/kayobe/latest/upgrading.html">https://docs.openstack.org/kayobe/latest/upgrading.html</a></p>
</div>
</div>
<div class="section" id="troubleshooting">
<h2>Troubleshooting<a class="headerlink" href="#troubleshooting" title="Permalink to this headline">¶</a></h2>
<div class="section" id="deploying-to-a-specific-hypervisor">
<h3>Deploying to a Specific Hypervisor<a class="headerlink" href="#deploying-to-a-specific-hypervisor" title="Permalink to this headline">¶</a></h3>
<p>To test creating an instance on a specific hypervisor, <em>as an admin-level user</em>
you can specify the hypervisor name as part of an extended availability zone
description.</p>
<p>To see the list of hypervisor names:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">admin# openstack hypervisor list</span>
</pre></div>
</div>
<p>To boot an instance on a specific hypervisor, for example on
<code class="docutils literal notranslate"><span class="pre">comp0</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">admin# openstack server create --flavor m1.tiny --network admin-vxlan --key-name &lt;key&gt; --image CentOS8.2 --availability-zone nova::comp0 vm-name</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="cleanup-procedures">
<h2>Cleanup Procedures<a class="headerlink" href="#cleanup-procedures" title="Permalink to this headline">¶</a></h2>
<p>OpenStack services can sometimes fail to remove all resources correctly. This
is the case with Magnum, which fails to clean up users in its domain after
clusters are deleted. <a class="reference external" href="https://review.opendev.org/#/q/Ibadd5b57fe175bb0b100266e2dbcc2e1ea4efcf9">A patch has been submitted to stable branches</a>.
Until this fix becomes available, if Magnum is in use, administrators can
perform the following cleanup procedure regularly:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">admin# for user in $(openstack user list --domain magnum -f value -c Name | grep -v magnum_trustee_domain_admin); do</span>
<span class="go">         if openstack coe cluster list -c uuid -f value | grep -q $(echo $user | sed &#39;s/_[0-9a-f]*$//&#39;); then</span>
<span class="go">           echo &quot;$user still in use, not deleting&quot;</span>
<span class="go">         else</span>
<span class="go">           openstack user delete --domain magnum $user</span>
<span class="go">         fi</span>
<span class="go">       done</span>
</pre></div>
</div>
</div>
<div class="section" id="elasticsearch-indexes-retention">
<h2>Elasticsearch indexes retention<a class="headerlink" href="#elasticsearch-indexes-retention" title="Permalink to this headline">¶</a></h2>
<p>To enable and alter default rotation values for Elasticsearch Curator edit <code class="docutils literal notranslate"><span class="pre">${KAYOBE_CONFIG_PATH}/kolla/globals.yml</span></code> - This applies both to Monasca and Central Logging configurations.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp"># </span>Allow Elasticsearch Curator to apply a retention policy to logs
<span class="go">enable_elasticsearch_curator: true</span>
<span class="gp"># </span>Duration after which index is closed
<span class="go">elasticsearch_curator_soft_retention_period_days: 90</span>
<span class="gp"># </span>Duration after which index is deleted
<span class="go">elasticsearch_curator_hard_retention_period_days: 180</span>
</pre></div>
</div>
<p>Reconfigure elasticsearch with new values:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kayobe overcloud service reconfigure --kolla-tags elasticsearch --kolla-skip-tags common --skip-precheck</span>
</pre></div>
</div>
<p>For more information see <a class="reference external" href="https://docs.openstack.org/kolla-ansible/ussuri/reference/logging-and-monitoring/central-logging-guide.html#curator">upstream documentation</a></p>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">OpenStack Administration Guide</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="working_with_openstack.html">Working with OpenStack</a></li>
<li class="toctree-l1"><a class="reference internal" href="working_with_kayobe.html">Working with Kayobe</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_inventory_management.html">Hardware Inventory Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="ceph_storage.html">Ceph Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="managing_users_and_projects.html">Managing Users and Projects</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Operations and Monitoring</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#access-to-kibana">Access to Kibana</a></li>
<li class="toctree-l2"><a class="reference internal" href="#access-to-grafana">Access to Grafana</a></li>
<li class="toctree-l2"><a class="reference internal" href="#migrating-virtual-machines">Migrating virtual machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="#openstack-reconfiguration">OpenStack Reconfiguration</a></li>
<li class="toctree-l2"><a class="reference internal" href="#backup-of-the-openstack-control-plane">Backup of the OpenStack Control Plane</a></li>
<li class="toctree-l2"><a class="reference internal" href="#control-plane-monitoring">Control Plane Monitoring</a></li>
<li class="toctree-l2"><a class="reference internal" href="#control-plane-shutdown-procedure">Control Plane Shutdown Procedure</a></li>
<li class="toctree-l2"><a class="reference internal" href="#control-plane-power-on-procedure">Control Plane Power on Procedure</a></li>
<li class="toctree-l2"><a class="reference internal" href="#software-updates">Software Updates</a></li>
<li class="toctree-l2"><a class="reference internal" href="#troubleshooting">Troubleshooting</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cleanup-procedures">Cleanup Procedures</a></li>
<li class="toctree-l2"><a class="reference internal" href="#elasticsearch-indexes-retention">Elasticsearch indexes retention</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="customising_deployment.html">Customising the OpenStack Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpus_in_openstack.html">Support for GPUs in OpenStack</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="managing_users_and_projects.html" title="previous chapter">Managing Users and Projects</a></li>
      <li>Next: <a href="customising_deployment.html" title="next chapter">Customising the OpenStack Deployment</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020, StackHPC Ltd.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.5.4</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/operations_and_monitoring.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>