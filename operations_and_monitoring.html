<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Operations and Monitoring &#8212; OpenStack Administration Guide  documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=039e1c02" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js?v=b3ba4146"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Wazuh Security Platform" href="wazuh.html" />
    <link rel="prev" title="Managing Users and Projects" href="managing_users_and_projects.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="operations-and-monitoring">
<h1>Operations and Monitoring<a class="headerlink" href="#operations-and-monitoring" title="Permalink to this heading">¶</a></h1>
<section id="access-to-kibana">
<h2>Access to Kibana<a class="headerlink" href="#access-to-kibana" title="Permalink to this heading">¶</a></h2>
<p>OpenStack control plane logs are aggregated from all servers by Fluentd and
stored in ElasticSearch. The control plane logs can be accessed from
ElasticSearch using Kibana, which is available at the following URL:
<a class="reference external" href="https://openstack.acme.example:5601">https://openstack.acme.example:5601</a></p>
<p>To log in, use the <code class="docutils literal notranslate"><span class="pre">kibana</span></code> user. The password is auto-generated by
Kolla-Ansible and can be extracted from the encrypted passwords file
(<a class="reference external" href="https://github.com/acme-openstack/kayobe-config/blob/acme/yoga/etc/kayobe/kolla/passwords.yml">https://github.com/acme-openstack/kayobe-config/blob/acme/yoga/etc/kayobe/kolla/passwords.yml</a>):</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kayobe# ansible-vault view ${KAYOBE_CONFIG_PATH}/kolla/passwords.yml --vault-password-file ~/vault-password | grep ^kibana</span>
</pre></div>
</div>
</section>
<section id="access-to-grafana">
<h2>Access to Grafana<a class="headerlink" href="#access-to-grafana" title="Permalink to this heading">¶</a></h2>
<p>Control plane metrics can be visualised in Grafana dashboards. Grafana can be
found at the following address: <a class="reference external" href="https://openstack.acme.example:3000">https://openstack.acme.example:3000</a></p>
<p>To log in, use the <code class="docutils literal notranslate"><span class="pre">grafana_local_admin</span></code> user. The password is auto-generated by
Kolla-Ansible and can be extracted from the encrypted passwords file
(<a class="reference external" href="https://github.com/acme-openstack/kayobe-config/blob/acme/yoga/etc/kayobe/kolla/passwords.yml">https://github.com/acme-openstack/kayobe-config/blob/acme/yoga/etc/kayobe/kolla/passwords.yml</a>):</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kayobe# ansible-vault view ${KAYOBE_CONFIG_PATH}/kolla/passwords.yml --vault-password-file ~/vault-password | grep ^grafana_admin_password</span>
</pre></div>
</div>
</section>
<section id="access-to-prometheus-alertmanager">
<span id="prometheus-alertmanager"></span><h2>Access to Prometheus Alertmanager<a class="headerlink" href="#access-to-prometheus-alertmanager" title="Permalink to this heading">¶</a></h2>
<p>Control plane alerts can be visualised and managed in Alertmanager, which can
be found at the following address: <a class="reference external" href="https://openstack.acme.example:9093">https://openstack.acme.example:9093</a></p>
<p>To log in, use the <code class="docutils literal notranslate"><span class="pre">admin</span></code> user. The password is auto-generated by
Kolla-Ansible and can be extracted from the encrypted passwords file
(<a class="reference external" href="https://github.com/acme-openstack/kayobe-config/blob/acme/yoga/etc/kayobe/kolla/passwords.yml">https://github.com/acme-openstack/kayobe-config/blob/acme/yoga/etc/kayobe/kolla/passwords.yml</a>):</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kayobe# ansible-vault view ${KAYOBE_CONFIG_PATH}/kolla/passwords.yml --vault-password-file ~/vault-password | grep ^prometheus_alertmanager_password</span>
</pre></div>
</div>
</section>
<section id="migrating-virtual-machines">
<h2>Migrating virtual machines<a class="headerlink" href="#migrating-virtual-machines" title="Permalink to this heading">¶</a></h2>
<p>To see where all virtual machines are running on the hypervisors:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">admin# openstack server list --all-projects --long</span>
</pre></div>
</div>
<p>To move a virtual machine with shared storage or booted from volume from one hypervisor to another, for example to
<code class="docutils literal notranslate"><span class="pre">comp0</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">admin# openstack --os-compute-api-version 2.30 server migrate --live-migration --host comp0 6a35592c-5a7e-4da3-9ab9-6765345641cb</span>
</pre></div>
</div>
<p>To move a virtual machine with local disks:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">admin# openstack --os-compute-api-version 2.30 server migrate --live-migration --block-migration --host comp0 6a35592c-5a7e-4da3-9ab9-6765345641cb</span>
</pre></div>
</div>
</section>
<section id="openstack-reconfiguration">
<h2>OpenStack Reconfiguration<a class="headerlink" href="#openstack-reconfiguration" title="Permalink to this heading">¶</a></h2>
<section id="disabling-a-service">
<h3>Disabling a Service<a class="headerlink" href="#disabling-a-service" title="Permalink to this heading">¶</a></h3>
<p>Ansible is oriented towards adding or reconfiguring services, but removing a
service is handled less well, because of Ansible’s imperative style.</p>
<p>To remove a service, it is disabled in Kayobe’s Kolla config, which prevents
other services from communicating with it. For example, to disable
<code class="docutils literal notranslate"><span class="pre">cinder-backup</span></code>, edit <code class="docutils literal notranslate"><span class="pre">${KAYOBE_CONFIG_PATH}/kolla.yml</span></code>:</p>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="gd">-enable_cinder_backup: true</span>
<span class="gi">+enable_cinder_backup: false</span>
</pre></div>
</div>
<p>Then, reconfigure Cinder services with Kayobe:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kayobe# kayobe overcloud service reconfigure --kolla-tags cinder</span>
</pre></div>
</div>
<p>However, the service itself, no longer in Ansible’s manifest of managed state,
must be manually stopped and prevented from restarting.</p>
<p>On each controller:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kayobe# docker rm -f cinder_backup</span>
</pre></div>
</div>
<p>Some services may store data in a dedicated Docker volume, which can be removed
with <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">volume</span> <span class="pre">rm</span></code>.</p>
</section>
<section id="installing-tls-certificates">
<h3>Installing TLS Certificates<a class="headerlink" href="#installing-tls-certificates" title="Permalink to this heading">¶</a></h3>
<p>TLS is implemented using a wildcard certificate available for <code class="docutils literal notranslate"><span class="pre">*.acme.example</span></code>.</p>
<p>To configure TLS for the first time, we write the contents of a PEM
file to the <code class="docutils literal notranslate"><span class="pre">secrets.yml</span></code> file as <code class="docutils literal notranslate"><span class="pre">secrets_kolla_external_tls_cert</span></code>.
Use a command of this form:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kayobe# ansible-vault edit ${KAYOBE_CONFIG_PATH}/secrets.yml --vault-password-file=~/vault-password</span>
</pre></div>
</div>
<p>Concatenate the contents of the certificate and key files to create
<code class="docutils literal notranslate"><span class="pre">secrets_kolla_external_tls_cert</span></code>.  The certificates should be installed in
this order:</p>
<ul class="simple">
<li><p>TLS certificate for the Acme OpenStack endpoint openstack.acme.example</p></li>
<li><p>Any intermediate certificates</p></li>
<li><p>The TLS certificate private key</p></li>
</ul>
<p>In <code class="docutils literal notranslate"><span class="pre">${KAYOBE_CONFIG_PATH}/kolla.yml</span></code>, set the following:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">kolla_enable_tls_external</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span>
<span class="nt">kolla_external_tls_cert</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;{{</span><span class="nv"> </span><span class="s">secrets_kolla_external_tls_cert</span><span class="nv"> </span><span class="s">}}&quot;</span>
</pre></div>
</div>
<p>To apply TLS configuration, we need to reconfigure all services, as endpoint URLs need to
be updated in Keystone:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kayobe# kayobe overcloud service reconfigure</span>
</pre></div>
</div>
<section id="alternative-configuration">
<h4>Alternative Configuration<a class="headerlink" href="#alternative-configuration" title="Permalink to this heading">¶</a></h4>
<p>As an alternative to writing the certificates as a variable to
<code class="docutils literal notranslate"><span class="pre">secrets.yml</span></code>, it is also possible to write the same data to a file,
<code class="docutils literal notranslate"><span class="pre">etc/kayobe/kolla/certificates/haproxy.pem</span></code>.  The file should be
vault-encrypted in the same manner as secrets.yml.  In this instance,
variable <code class="docutils literal notranslate"><span class="pre">kolla_external_tls_cert</span></code> does not need to be defined.</p>
<p>See <a class="reference external" href="https://docs.openstack.org/kolla-ansible/latest/admin/tls.html">Kolla-Ansible TLS guide</a> for
further details.</p>
</section>
</section>
<section id="updating-tls-certificates">
<h3>Updating TLS Certificates<a class="headerlink" href="#updating-tls-certificates" title="Permalink to this heading">¶</a></h3>
<p>Check the expiry date on an installed TLS certificate from a host that can
reach the Acme OpenStack APIs:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">openstack# openssl s_client -connect openstack.acme.example:443 2&gt; /dev/null | openssl x509 -noout -dates</span>
</pre></div>
</div>
<p><em>NOTE</em>: Prometheus Blackbox monitoring can check certificates automatically
and alert when expiry is approaching.</p>
<p>To update an existing certificate, for example when it has reached expiration,
change the value of <code class="docutils literal notranslate"><span class="pre">secrets_kolla_external_tls_cert</span></code>, in the same order as
above.  Run the following command:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kayobe# kayobe overcloud service reconfigure --kolla-tags haproxy</span>
</pre></div>
</div>
</section>
<section id="taking-a-hypervisor-out-of-service">
<span id="id1"></span><h3>Taking a Hypervisor out of Service<a class="headerlink" href="#taking-a-hypervisor-out-of-service" title="Permalink to this heading">¶</a></h3>
<p>To take a hypervisor out of Nova scheduling, for example <code class="docutils literal notranslate"><span class="pre">comp0</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">admin# openstack compute service set --disable \</span>
<span class="go">       comp0 nova-compute</span>
</pre></div>
</div>
<p>Running instances on the hypervisor will not be affected, but new instances
will not be deployed on it.</p>
<p>A reason for disabling a hypervisor can be documented with the
<code class="docutils literal notranslate"><span class="pre">--disable-reason</span></code> flag:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">admin# openstack compute service set --disable \</span>
<span class="go">       --disable-reason &quot;Broken drive&quot; comp0 nova-compute</span>
</pre></div>
</div>
<p>Details about all hypervisors and the reasons they are disabled can be
displayed with:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">admin# openstack compute service list --long</span>
</pre></div>
</div>
<p>And then to enable a hypervisor again:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">admin# openstack compute service set --enable \</span>
<span class="go">       comp0 nova-compute</span>
</pre></div>
</div>
</section>
<section id="managing-space-in-the-docker-registry">
<h3>Managing Space in the Docker Registry<a class="headerlink" href="#managing-space-in-the-docker-registry" title="Permalink to this heading">¶</a></h3>
<p>If the Docker registry becomes full, this can prevent container updates and
(depending on the storage configuration of the seed host) could lead to other
problems with services provided by the seed host.</p>
<p>To remove container images from the Docker Registry, follow this process:</p>
<ul class="simple">
<li><p>Reconfigure the registry container to allow deleting containers. This can be
done in <code class="docutils literal notranslate"><span class="pre">docker-registry.yml</span></code> with Kayobe:</p></li>
</ul>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">docker_registry_env</span><span class="p">:</span>
<span class="w">  </span><span class="nt">REGISTRY_STORAGE_DELETE_ENABLED</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;true&quot;</span>
</pre></div>
</div>
<ul class="simple">
<li><p>For the change to take effect, run:</p></li>
</ul>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kayobe seed host configure</span>
</pre></div>
</div>
<ul class="simple">
<li><p>A helper script is useful, such as <a class="reference external" href="https://github.com/byrnedo/docker-reg-tool">https://github.com/byrnedo/docker-reg-tool</a>
(this requires <code class="docutils literal notranslate"><span class="pre">jq</span></code>). To delete all images with a specific tag, use:</p></li>
</ul>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">for repo in `./docker_reg_tool http://registry-ip:4000 list`; do</span>
<span class="go">     ./docker_reg_tool http://registry-ip:4000 delete $repo $tag</span>
<span class="go">done</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Deleting the tag does not actually release the space. To actually free up
space, run garbage collection:</p></li>
</ul>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">seed# docker exec docker_registry bin/registry garbage-collect /etc/docker/registry/config.yml</span>
</pre></div>
</div>
<p>The seed host can also accrue a lot of data from building container images.
The images stored locally in the seed host can be seen using <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">image</span> <span class="pre">ls</span></code>.</p>
<p>Old and redundant images can be identified from their names and tags, and
removed using <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">image</span> <span class="pre">rm</span></code>.</p>
</section>
</section>
<section id="backup-of-the-openstack-control-plane">
<h2>Backup of the OpenStack Control Plane<a class="headerlink" href="#backup-of-the-openstack-control-plane" title="Permalink to this heading">¶</a></h2>
<p>As the backup procedure is constantly changing, it is normally best to check
the upstream documentation for an up to date procedure. Here is a high level
overview of the key things you need to backup:</p>
<section id="controllers">
<h3>Controllers<a class="headerlink" href="#controllers" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.openstack.org/kayobe/latest/administration/overcloud.html#performing-database-backups">Back up SQL databases</a></p></li>
<li><p><a class="reference external" href="https://docs.openstack.org/kayobe/latest/administration/overcloud.html#saving-overcloud-service-configuration">Back up configuration in /etc/kolla</a></p></li>
</ul>
</section>
<section id="compute">
<h3>Compute<a class="headerlink" href="#compute" title="Permalink to this heading">¶</a></h3>
<p>The compute nodes can largely be thought of as ephemeral, but you do need to
make sure you have migrated any instances and disabled the hypervisor before
decommissioning or making any disruptive configuration change.</p>
</section>
<section id="monitoring">
<h3>Monitoring<a class="headerlink" href="#monitoring" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.influxdata.com/influxdb/v1.8/administration/backup_and_restore/">Back up InfluxDB</a></p></li>
<li><p><a class="reference external" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/backup-cluster-data.html">Back up ElasticSearch</a></p></li>
<li><p><a class="reference external" href="https://prometheus.io/docs/prometheus/latest/querying/api/#snapshot">Back up Prometheus</a></p></li>
</ul>
</section>
<section id="seed">
<h3>Seed<a class="headerlink" href="#seed" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.openstack.org/kayobe/latest/administration/seed.html#database-backup-restore">Back up bifrost</a></p></li>
</ul>
</section>
<section id="ansible-control-host">
<h3>Ansible control host<a class="headerlink" href="#ansible-control-host" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p>Back up service VMs such as the seed VM</p></li>
</ul>
</section>
</section>
<section id="control-plane-monitoring">
<h2>Control Plane Monitoring<a class="headerlink" href="#control-plane-monitoring" title="Permalink to this heading">¶</a></h2>
<p>The control plane has been configured to collect logs centrally using the EFK
stack (Elasticsearch, Fluentd and Kibana).</p>
<p>Telemetry monitoring of the control plane is performed by Prometheus. Metrics
are collected by Prometheus exporters, which are either running on all hosts
(e.g.  node exporter), on specific hosts (e.g. controllers for the memcached
exporter or monitoring hosts for the OpenStack exporter). These exporters are
scraped by the Prometheus server.</p>
<section id="configuring-prometheus-alerts">
<h3>Configuring Prometheus Alerts<a class="headerlink" href="#configuring-prometheus-alerts" title="Permalink to this heading">¶</a></h3>
<p>Alerts are defined in code and stored in Kayobe configuration. See <code class="docutils literal notranslate"><span class="pre">*.rules</span></code>
files in <code class="docutils literal notranslate"><span class="pre">${KAYOBE_CONFIG_PATH}/kolla/config/prometheus</span></code> as a model to add
custom rules.</p>
</section>
<section id="silencing-prometheus-alerts">
<h3>Silencing Prometheus Alerts<a class="headerlink" href="#silencing-prometheus-alerts" title="Permalink to this heading">¶</a></h3>
<p>Sometimes alerts must be silenced because the root cause cannot be resolved
right away, such as when hardware is faulty. For example, an unreachable
hypervisor will produce several alerts:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">InstanceDown</span></code> from Node Exporter</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">OpenStackServiceDown</span></code> from the OpenStack exporter, which reports status of
the <code class="docutils literal notranslate"><span class="pre">nova-compute</span></code> agent on the host</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">PrometheusTargetMissing</span></code> from several Prometheus exporters</p></li>
</ul>
<p>Rather than silencing each alert one by one for a specific host, a silence can
apply to multiple alerts using a reduced list of labels. <a class="reference internal" href="#prometheus-alertmanager"><span class="std std-ref">Log into
Alertmanager</span></a>, click on the <code class="docutils literal notranslate"><span class="pre">Silence</span></code> button next
to an alert and adjust the matcher list to keep only <code class="docutils literal notranslate"><span class="pre">instance=&lt;hostname&gt;</span></code>
label.  Then, create another silence to match <code class="docutils literal notranslate"><span class="pre">hostname=&lt;hostname&gt;</span></code> (this is
required because, for the OpenStack exporter, the instance is the host running
the monitoring service rather than the host being monitored).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>After creating the silence, you may get redirected to a 404 page. This is a
<a class="reference external" href="https://github.com/prometheus/alertmanager/issues/1377">known issue</a>
when running several Alertmanager instances behind HAProxy.</p>
</div>
<section id="generating-alerts-from-metrics">
<h4>Generating Alerts from Metrics<a class="headerlink" href="#generating-alerts-from-metrics" title="Permalink to this heading">¶</a></h4>
<p>Alerts are defined in code and stored in Kayobe configuration. See <code class="docutils literal notranslate"><span class="pre">*.rules</span></code>
files in <code class="docutils literal notranslate"><span class="pre">${KAYOBE_CONFIG_PATH}/kolla/config/prometheus</span></code> as a model to add
custom rules.</p>
</section>
</section>
</section>
<section id="control-plane-shutdown-procedure">
<h2>Control Plane Shutdown Procedure<a class="headerlink" href="#control-plane-shutdown-procedure" title="Permalink to this heading">¶</a></h2>
<section id="overview">
<h3>Overview<a class="headerlink" href="#overview" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p>Verify integrity of clustered components (RabbitMQ, Galera, Keepalived). They
should all report a healthy status.</p></li>
<li><p>Put node into maintenance mode in bifrost to prevent it from automatically
powering back on</p></li>
<li><p>Shutdown down nodes one at a time gracefully using systemctl poweroff</p></li>
</ul>
</section>
<section id="id2">
<h3>Controllers<a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h3>
<p>If you are restarting the controllers, it is best to do this one controller at
a time to avoid the clustered components losing quorum.</p>
<section id="checking-galera-state">
<h4>Checking Galera state<a class="headerlink" href="#checking-galera-state" title="Permalink to this heading">¶</a></h4>
<p>On each controller perform the following:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">[stack@ctrl0 ~]$ </span>docker<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>-i<span class="w"> </span>mariadb<span class="w"> </span>mysql<span class="w"> </span>-u<span class="w"> </span>root<span class="w"> </span>-p<span class="w"> </span>-e<span class="w"> </span><span class="s2">&quot;SHOW STATUS LIKE &#39;wsrep_local_state_comment&#39;&quot;</span>
<span class="go">Variable_name   Value</span>
<span class="go">wsrep_local_state_comment       Synced</span>
</pre></div>
</div>
<p>The password can be found using:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kayobe# ansible-vault view ${KAYOBE_CONFIG_PATH}/kolla/passwords.yml \</span>
<span class="go">        --vault-password-file ~/vault-password | grep ^database</span>
</pre></div>
</div>
</section>
<section id="checking-rabbitmq">
<h4>Checking RabbitMQ<a class="headerlink" href="#checking-rabbitmq" title="Permalink to this heading">¶</a></h4>
<p>RabbitMQ health is determined using the command <code class="docutils literal notranslate"><span class="pre">rabbitmqctl</span> <span class="pre">cluster_status</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">[stack@ctrl0 ~]$ </span>docker<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>rabbitmq<span class="w"> </span>rabbitmqctl<span class="w"> </span>cluster_status
<span class="go">Cluster status of node rabbit@ctrl0 ...</span>
<span class="go">[{nodes,[{disc,[&#39;rabbit@ctrl0&#39;,&#39;rabbit@ctrl1&#39;,</span>
<span class="go">                &#39;rabbit@ctr2&#39;]}]},</span>
<span class="go"> {running_nodes,[&#39;rabbit@ctrl1&#39;,&#39;rabbit@ctr2&#39;,</span>
<span class="go">                 &#39;rabbit@ctrl0&#39;]},</span>
<span class="go"> {cluster_name,&lt;&lt;&quot;rabbit@ctr2&quot;&gt;&gt;},</span>
<span class="go"> {partitions,[]},</span>
<span class="go"> {alarms,[{&#39;rabbit@ctrl1&#39;,[]},</span>
<span class="go">          {&#39;rabbit@ctr2&#39;,[]},</span>
<span class="go">          {&#39;rabbit@ctrl0&#39;,[]}]}]</span>
</pre></div>
</div>
</section>
<section id="checking-keepalived">
<h4>Checking Keepalived<a class="headerlink" href="#checking-keepalived" title="Permalink to this heading">¶</a></h4>
<p>On (for example) three controllers:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">[stack@ctrl0 ~]$ </span>docker<span class="w"> </span>logs<span class="w"> </span>keepalived
</pre></div>
</div>
<p>Two instances should show:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">VRRP_Instance(kolla_internal_vip_51) Entering BACKUP STATE</span>
</pre></div>
</div>
<p>and the other:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">VRRP_Instance(kolla_internal_vip_51) Entering MASTER STATE</span>
</pre></div>
</div>
</section>
</section>
<section id="id3">
<h3>Ansible Control Host<a class="headerlink" href="#id3" title="Permalink to this heading">¶</a></h3>
<p>The Ansible control host is not enrolled in bifrost. This node may run services
such as the seed virtual machine which will need to be gracefully powered down.</p>
</section>
<section id="id4">
<h3>Compute<a class="headerlink" href="#id4" title="Permalink to this heading">¶</a></h3>
<p>If you are shutting down a single hypervisor, to avoid down time to tenants it
is advisable to migrate all of the instances to another machine. See
<a class="reference internal" href="hardware_inventory_management.html#evacuating-all-instances"><span class="std std-ref">Evacuating all instances</span></a>.</p>
</section>
<section id="shutting-down-the-seed-vm">
<h3>Shutting down the seed VM<a class="headerlink" href="#shutting-down-the-seed-vm" title="Permalink to this heading">¶</a></h3>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kayobe# virsh shutdown acme-seed</span>
</pre></div>
</div>
</section>
<section id="full-shutdown">
<span id="id5"></span><h3>Full shutdown<a class="headerlink" href="#full-shutdown" title="Permalink to this heading">¶</a></h3>
<p>In case a full shutdown of the system is required, we advise to use the
following order:</p>
<ul class="simple">
<li><p>Perform a graceful shutdown of all virtual machine instances</p></li>
<li><p>Shut down compute nodes</p></li>
<li><p>Shut down monitoring node</p></li>
<li><p>Shut down network nodes (if separate from controllers)</p></li>
<li><p>Shut down controllers</p></li>
<li><p>Shut down Ceph nodes (if applicable)</p></li>
<li><p>Shut down seed VM</p></li>
<li><p>Shut down Ansible control host</p></li>
</ul>
</section>
<section id="rebooting-a-node">
<h3>Rebooting a node<a class="headerlink" href="#rebooting-a-node" title="Permalink to this heading">¶</a></h3>
<p>Example: Reboot all compute hosts apart from <code class="docutils literal notranslate"><span class="pre">comp0</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kayobe# kayobe overcloud host command run --limit &#39;compute:!comp0&#39; -b --command &quot;shutdown -r&quot;</span>
</pre></div>
</div>
</section>
<section id="references">
<h3>References<a class="headerlink" href="#references" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://galeracluster.com/library/training/tutorials/restarting-cluster.html">https://galeracluster.com/library/training/tutorials/restarting-cluster.html</a></p></li>
</ul>
</section>
</section>
<section id="control-plane-power-on-procedure">
<h2>Control Plane Power on Procedure<a class="headerlink" href="#control-plane-power-on-procedure" title="Permalink to this heading">¶</a></h2>
<section id="id6">
<h3>Overview<a class="headerlink" href="#id6" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p>Remove the node from maintenance mode in bifrost</p></li>
<li><p>Bifrost should automatically power on the node via IPMI</p></li>
<li><p>Check that all docker containers are running</p></li>
<li><p>Check Kibana for any messages with log level ERROR or equivalent</p></li>
</ul>
</section>
<section id="id7">
<h3>Controllers<a class="headerlink" href="#id7" title="Permalink to this heading">¶</a></h3>
<p>If all of the servers were shut down at the same time, it is necessary to run a
script to recover the database once they have all started up. This can be done
with the following command:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kayobe# kayobe overcloud database recover</span>
</pre></div>
</div>
</section>
<section id="id8">
<h3>Ansible Control Host<a class="headerlink" href="#id8" title="Permalink to this heading">¶</a></h3>
<p>The Ansible control host is not enrolled in Bifrost and will have to be powered
on manually.</p>
</section>
<section id="seed-vm">
<h3>Seed VM<a class="headerlink" href="#seed-vm" title="Permalink to this heading">¶</a></h3>
<p>The seed VM (and any other service VM) should start automatically when the seed
hypervisor is powered on. If it does not, it can be started with:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kayobe# virsh start seed-0</span>
</pre></div>
</div>
</section>
<section id="full-power-on">
<h3>Full power on<a class="headerlink" href="#full-power-on" title="Permalink to this heading">¶</a></h3>
<p>Follow the order in <a class="reference internal" href="#full-shutdown"><span class="std std-ref">Full shutdown</span></a>, but in reverse order.</p>
</section>
<section id="shutting-down-restarting-monitoring-services">
<h3>Shutting Down / Restarting Monitoring Services<a class="headerlink" href="#shutting-down-restarting-monitoring-services" title="Permalink to this heading">¶</a></h3>
<section id="shutting-down">
<h4>Shutting down<a class="headerlink" href="#shutting-down" title="Permalink to this heading">¶</a></h4>
<p>Log into the monitoring host(s):</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kayobe# ssh stack@mon0</span>
</pre></div>
</div>
<p>Stop all Docker containers:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">mon0# for i in `docker ps -q`; do docker stop $i; done</span>
</pre></div>
</div>
<p>Shut down the node:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">mon0# sudo shutdown -h</span>
</pre></div>
</div>
</section>
<section id="starting-up">
<h4>Starting up<a class="headerlink" href="#starting-up" title="Permalink to this heading">¶</a></h4>
<p>The monitoring services containers will automatically start when the monitoring
node is powered back on.</p>
</section>
</section>
</section>
<section id="software-updates">
<h2>Software Updates<a class="headerlink" href="#software-updates" title="Permalink to this heading">¶</a></h2>
<section id="update-packages-on-control-plane">
<h3>Update Packages on Control Plane<a class="headerlink" href="#update-packages-on-control-plane" title="Permalink to this heading">¶</a></h3>
<p>OS packages can be updated with:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kayobe # kayobe overcloud host package update --limit comp0 --packages &#39;*&#39;</span>
<span class="go">kayobe # kayobe overcloud seed package update --packages &#39;*&#39;</span>
</pre></div>
</div>
<p>See <a class="reference external" href="https://docs.openstack.org/kayobe/latest/administration/overcloud.html#updating-packages">https://docs.openstack.org/kayobe/latest/administration/overcloud.html#updating-packages</a></p>
</section>
<section id="minor-upgrades-to-openstack-services">
<h3>Minor Upgrades to OpenStack Services<a class="headerlink" href="#minor-upgrades-to-openstack-services" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p>Pull latest changes from upstream stable branch to your own <code class="docutils literal notranslate"><span class="pre">kolla</span></code> fork (if applicable)</p></li>
<li><p>Update <code class="docutils literal notranslate"><span class="pre">kolla_openstack_release</span></code> in <code class="docutils literal notranslate"><span class="pre">etc/kayobe/kolla.yml</span></code> (unless using default)</p></li>
<li><p>Update tags for the images in <code class="docutils literal notranslate"><span class="pre">etc/kayobe/kolla/globals.yml</span></code> to use the new value of <code class="docutils literal notranslate"><span class="pre">kolla_openstack_release</span></code></p></li>
<li><p>Rebuild container images</p></li>
<li><p>Pull container images to overcloud hosts</p></li>
<li><p>Run kayobe overcloud service upgrade</p></li>
</ul>
<p>For more information, see: <a class="reference external" href="https://docs.openstack.org/kayobe/latest/upgrading.html">https://docs.openstack.org/kayobe/latest/upgrading.html</a></p>
</section>
</section>
<section id="troubleshooting">
<h2>Troubleshooting<a class="headerlink" href="#troubleshooting" title="Permalink to this heading">¶</a></h2>
<section id="deploying-to-a-specific-hypervisor">
<h3>Deploying to a Specific Hypervisor<a class="headerlink" href="#deploying-to-a-specific-hypervisor" title="Permalink to this heading">¶</a></h3>
<p>To test creating an instance on a specific hypervisor, <em>as an admin-level user</em>
you can specify the hypervisor name as part of an extended availability zone
description.</p>
<p>To see the list of hypervisor names:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">admin# openstack hypervisor list</span>
</pre></div>
</div>
<p>To boot an instance on a specific hypervisor, for example on
<code class="docutils literal notranslate"><span class="pre">comp0</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">admin# openstack server create --flavor m1.tiny --network admin-vxlan --key-name &lt;key&gt; --image CentOS8.2 --availability-zone nova::comp0 vm-name</span>
</pre></div>
</div>
</section>
</section>
<section id="cleanup-procedures">
<h2>Cleanup Procedures<a class="headerlink" href="#cleanup-procedures" title="Permalink to this heading">¶</a></h2>
<p>OpenStack services can sometimes fail to remove all resources correctly. This
is the case with Magnum, which fails to clean up users in its domain after
clusters are deleted. <a class="reference external" href="https://review.opendev.org/#/q/Ibadd5b57fe175bb0b100266e2dbcc2e1ea4efcf9">A patch has been submitted to stable branches</a>.
Until this fix becomes available, if Magnum is in use, administrators can
perform the following cleanup procedure regularly:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">admin# for user in $(openstack user list --domain magnum -f value -c Name | grep -v magnum_trustee_domain_admin); do</span>
<span class="go">         if openstack coe cluster list -c uuid -f value | grep -q $(echo $user | sed &#39;s/_[0-9a-f]*$//&#39;); then</span>
<span class="go">           echo &quot;$user still in use, not deleting&quot;</span>
<span class="go">         else</span>
<span class="go">           openstack user delete --domain magnum $user</span>
<span class="go">         fi</span>
<span class="go">       done</span>
</pre></div>
</div>
</section>
<section id="elasticsearch-indexes-retention">
<h2>Elasticsearch indexes retention<a class="headerlink" href="#elasticsearch-indexes-retention" title="Permalink to this heading">¶</a></h2>
<p>To enable and alter default rotation values for Elasticsearch Curator, edit
<code class="docutils literal notranslate"><span class="pre">${KAYOBE_CONFIG_PATH}/kolla/globals.yml</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp"># </span>Allow<span class="w"> </span>Elasticsearch<span class="w"> </span>Curator<span class="w"> </span>to<span class="w"> </span>apply<span class="w"> </span>a<span class="w"> </span>retention<span class="w"> </span>policy<span class="w"> </span>to<span class="w"> </span>logs
<span class="go">enable_elasticsearch_curator: true</span>

<span class="gp"># </span>Duration<span class="w"> </span>after<span class="w"> </span>which<span class="w"> </span>index<span class="w"> </span>is<span class="w"> </span>closed
<span class="go">elasticsearch_curator_soft_retention_period_days: 90</span>

<span class="gp"># </span>Duration<span class="w"> </span>after<span class="w"> </span>which<span class="w"> </span>index<span class="w"> </span>is<span class="w"> </span>deleted
<span class="go">elasticsearch_curator_hard_retention_period_days: 180</span>
</pre></div>
</div>
<p>Reconfigure Elasticsearch with new values:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kayobe overcloud service reconfigure --kolla-tags elasticsearch</span>
</pre></div>
</div>
<p>For more information see the <a class="reference external" href="https://docs.openstack.org/kolla-ansible/latest/reference/logging-and-monitoring/central-logging-guide.html#curator">upstream documentation</a>.</p>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">OpenStack Administration Guide</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="working_with_openstack.html">Working with OpenStack</a></li>
<li class="toctree-l1"><a class="reference internal" href="working_with_kayobe.html">Working with Kayobe</a></li>
<li class="toctree-l1"><a class="reference internal" href="physical_network.html">Physical network</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_inventory_management.html">Hardware Inventory Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="ceph_storage.html">Ceph Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="managing_users_and_projects.html">Managing Users and Projects</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Operations and Monitoring</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#access-to-kibana">Access to Kibana</a></li>
<li class="toctree-l2"><a class="reference internal" href="#access-to-grafana">Access to Grafana</a></li>
<li class="toctree-l2"><a class="reference internal" href="#access-to-prometheus-alertmanager">Access to Prometheus Alertmanager</a></li>
<li class="toctree-l2"><a class="reference internal" href="#migrating-virtual-machines">Migrating virtual machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="#openstack-reconfiguration">OpenStack Reconfiguration</a></li>
<li class="toctree-l2"><a class="reference internal" href="#backup-of-the-openstack-control-plane">Backup of the OpenStack Control Plane</a></li>
<li class="toctree-l2"><a class="reference internal" href="#control-plane-monitoring">Control Plane Monitoring</a></li>
<li class="toctree-l2"><a class="reference internal" href="#control-plane-shutdown-procedure">Control Plane Shutdown Procedure</a></li>
<li class="toctree-l2"><a class="reference internal" href="#control-plane-power-on-procedure">Control Plane Power on Procedure</a></li>
<li class="toctree-l2"><a class="reference internal" href="#software-updates">Software Updates</a></li>
<li class="toctree-l2"><a class="reference internal" href="#troubleshooting">Troubleshooting</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cleanup-procedures">Cleanup Procedures</a></li>
<li class="toctree-l2"><a class="reference internal" href="#elasticsearch-indexes-retention">Elasticsearch indexes retention</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="wazuh.html">Wazuh Security Platform</a></li>
<li class="toctree-l1"><a class="reference internal" href="customising_deployment.html">Customising the OpenStack Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpus_in_openstack.html">Support for GPUs in OpenStack</a></li>
<li class="toctree-l1"><a class="reference internal" href="baremetal_management.html">Bare Metal Compute Hardware Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="rally_and_tempest.html">Verifying the Cloud with Rally and Tempest</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="managing_users_and_projects.html" title="previous chapter">Managing Users and Projects</a></li>
      <li>Next: <a href="wazuh.html" title="next chapter">Wazuh Security Platform</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020-2023, StackHPC Ltd.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="_sources/operations_and_monitoring.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>